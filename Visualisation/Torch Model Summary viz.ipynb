{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 17371,
     "status": "ok",
     "timestamp": 1733683933271,
     "user": {
      "displayName": "Alexander Caning",
      "userId": "11690755251269761455"
     },
     "user_tz": -60
    },
    "id": "AfBPiRUEO3Wx",
    "outputId": "ae6a1e2b-ca2d-4cfe-e3ad-12e54c3e5d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "Collecting xLSTM\n",
      "  Downloading xlstm-1.0.8-py3-none-any.whl.metadata (19 kB)\n",
      "Downloading xlstm-1.0.8-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xLSTM\n",
      "Successfully installed xLSTM-1.0.8\n",
      "Collecting ninja\n",
      "  Downloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
      "Downloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ninja\n",
      "Successfully installed ninja-1.11.1.2\n",
      "Collecting xlstm\n",
      "  Downloading xlstm-1.0.8-py3-none-any.whl.metadata (19 kB)\n",
      "Downloading xlstm-1.0.8-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xlstm\n",
      "  Attempting uninstall: xlstm\n",
      "    Found existing installation: xlstm 1.0.8\n",
      "    Uninstalling xlstm-1.0.8:\n",
      "      Successfully uninstalled xlstm-1.0.8\n",
      "Successfully installed xlstm-1.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo\n",
    "!pip install xLSTM\n",
    "!pip install ninja\n",
    "!pip install xlstm --force-reinstall --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7406,
     "status": "ok",
     "timestamp": 1733683940672,
     "user": {
      "displayName": "Alexander Caning",
      "userId": "11690755251269761455"
     },
     "user_tz": -60
    },
    "id": "IQvnM20OOqDW"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pc04J3anPatM"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1733683940673,
     "user": {
      "displayName": "Alexander Caning",
      "userId": "11690755251269761455"
     },
     "user_tz": -60
    },
    "id": "nHkikvnaO7wq"
   },
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim, # Number of features - we have 1 because music!\n",
    "                 hidden_size, # Hidden size of LSTM layer\n",
    "                 num_layers, # Number of LSTM layers\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=95,\n",
    "                                      embedding_dim=embedding_dim)\n",
    "\n",
    "        self.LSTM = nn.LSTM(input_size=embedding_dim,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          batch_first=True)\n",
    "\n",
    "        self.dp1 = nn.Dropout(0.5)\n",
    "        self.dp2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size,\n",
    "                             hidden_size)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, 95) # hidden size x vocab size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h_n, _) = self.LSTM(x) # Needs N x seq_L x FEATURE_DIM\n",
    "        x = h_n[-1]\n",
    "        x = self.dp1(x)\n",
    "        x = self.fc1(x) # See w8_forecast solution for an explanation of slicing\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 603,
     "status": "ok",
     "timestamp": 1733683941271,
     "user": {
      "displayName": "Alexander Caning",
      "userId": "11690755251269761455"
     },
     "user_tz": -60
    },
    "id": "y1mPKPZqO_tG",
    "outputId": "748a1b62-afa9-43d3-d5b2-a4da77f7d8f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimpleModel                              [1, 95]                   --\n",
       "├─Embedding: 1-1                         [1, 128, 128]             12,160\n",
       "├─LSTM: 1-2                              [1, 128, 256]             1,974,272\n",
       "├─Dropout: 1-3                           [1, 256]                  --\n",
       "├─Linear: 1-4                            [1, 256]                  65,792\n",
       "├─Dropout: 1-5                           [1, 256]                  --\n",
       "├─Linear: 1-6                            [1, 95]                   24,415\n",
       "==========================================================================================\n",
       "Total params: 2,076,639\n",
       "Trainable params: 2,076,639\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 252.81\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.40\n",
       "Params size (MB): 8.31\n",
       "Estimated Total Size (MB): 8.70\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleModel(128, 256, 4)\n",
    "summary(model, input_size=(1, 128), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPQZsKLHPh7H"
   },
   "source": [
    "# xLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1119,
     "status": "ok",
     "timestamp": 1733683942386,
     "user": {
      "displayName": "Alexander Caning",
      "userId": "11690755251269761455"
     },
     "user_tz": -60
    },
    "id": "wE25udfdPyJo",
    "outputId": "799c072b-cf15-41b6-de0a-1f0ae99806a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig, mLSTMBlockConfig, mLSTMLayerConfig, sLSTMBlockConfig, sLSTMLayerConfig, FeedForwardConfig\n",
    "\n",
    "class SimpleModelWithxLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embedding_dim,\n",
    "                 hidden_size,\n",
    "                 context_length,\n",
    "                 num_blocks,\n",
    "                 slstm_at,\n",
    "                 dropout_prob = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "\n",
    "        # xLSTM configuration\n",
    "        self.xLSTM_cfg = xLSTMBlockStackConfig(\n",
    "            mlstm_block=mLSTMBlockConfig(\n",
    "                mlstm=mLSTMLayerConfig(\n",
    "                    conv1d_kernel_size=4,\n",
    "                    qkv_proj_blocksize=4,\n",
    "                    num_heads=4\n",
    "                )\n",
    "            ),\n",
    "            slstm_block=sLSTMBlockConfig(\n",
    "                slstm=sLSTMLayerConfig(\n",
    "                    backend=\"vanilla\",\n",
    "                    num_heads=4,\n",
    "                    conv1d_kernel_size=4,\n",
    "                    bias_init=\"powerlaw_blockdependent\",\n",
    "                ),\n",
    "                feedforward=FeedForwardConfig(\n",
    "                    proj_factor=1.3,\n",
    "                    act_fn=\"gelu\"\n",
    "                ),\n",
    "            ),\n",
    "            context_length=context_length,\n",
    "            num_blocks=num_blocks,\n",
    "            embedding_dim=embedding_dim,\n",
    "            slstm_at=slstm_at,\n",
    "        )\n",
    "\n",
    "        # Initialize xLSTM stack\n",
    "        self.xLSTM = xLSTMBlockStack(self.xLSTM_cfg)\n",
    "\n",
    "        self.dropout_1 = nn.Dropout(dropout_prob)\n",
    "\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_size)\n",
    "        self.dropout_2 = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed the input\n",
    "        x = self.embedding(x)  # Shape: [batch_size, seq_length, embedding_dim]\n",
    "\n",
    "        # Pass through the xLSTM stack\n",
    "        x = self.xLSTM(x)  # Shape: [batch_size, seq_length, embedding_dim]\n",
    "\n",
    "        x = self.dropout_1(x)\n",
    "        # Take the last sequence step (e.g., for classification tasks)\n",
    "        x = x[:, -1, :]  # Shape: [batch_size, embedding_dim]\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)  # Shape: [batch_size, hidden_size]\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.fc2(x)  # Shape: [batch_size, vocab_size]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1733683942817,
     "user": {
      "displayName": "Alexander Caning",
      "userId": "11690755251269761455"
     },
     "user_tz": -60
    },
    "id": "GjF2xc2hQADk",
    "outputId": "e0c2f214-2673-4e8c-90ed-5e36a87ceea3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "SimpleModelWithxLSTM                                    [1, 95]                   --\n",
       "├─Embedding: 1-1                                        [1, 128, 128]             12,160\n",
       "├─xLSTMBlockStack: 1-2                                  [1, 128, 128]             --\n",
       "│    └─ModuleList: 2-1                                  --                        --\n",
       "│    │    └─mLSTMBlock: 3-1                             [1, 128, 128]             109,448\n",
       "│    │    └─sLSTMBlock: 3-2                             [1, 128, 128]             108,032\n",
       "│    │    └─mLSTMBlock: 3-3                             [1, 128, 128]             109,448\n",
       "│    │    └─mLSTMBlock: 3-4                             [1, 128, 128]             109,448\n",
       "│    └─LayerNorm: 2-2                                   [1, 128, 128]             128\n",
       "├─Dropout: 1-3                                          [1, 128, 128]             --\n",
       "├─Linear: 1-4                                           [1, 256]                  33,024\n",
       "├─Dropout: 1-5                                          [1, 256]                  --\n",
       "├─Linear: 1-6                                           [1, 95]                   24,415\n",
       "=========================================================================================================\n",
       "Total params: 506,103\n",
       "Trainable params: 506,103\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.07\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 8.31\n",
       "Params size (MB): 2.02\n",
       "Estimated Total Size (MB): 10.33\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleModelWithxLSTM(\n",
    "    vocab_size=95,\n",
    "    embedding_dim=128,\n",
    "    hidden_size=256,\n",
    "    context_length=128,\n",
    "    num_blocks=4,\n",
    "    slstm_at=[1]\n",
    ")\n",
    "summary(model, input_size = (1, 128), dtypes=[torch.long])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyML06DrY+aa2rXDOKbBN8Cx",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
